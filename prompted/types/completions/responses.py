"""
prompted.types.completions.responses
"""

from typing import (
    Any,
    List,
    Union,
    Literal,
    Optional,
)
from pydantic import BaseModel, Field

from .messages import CompletionMessageContentPart


class Subscriptable(BaseModel):
    """
    A light wrapper over a Pydantic BaseModel, that allows for subscriptable
    access to fields in a model.
    """

    def __getitem__(self, key: str) -> Any:
        """
        Get an item from the model.
        """
        return getattr(self, key)

    def __setitem__(self, key: str, value: Any) -> None:
        """
        Set an item in the model.
        """
        setattr(self, key, value)

    def __contains__(self, key: str) -> bool:
        """
        Check if a key exists in the model.
        """
        # Check if the field is set or has a default value
        if key in self.model_fields_set:
            return True
        if key in self.model_fields and self.model_fields[key].default is not None:
            return True
        # For aliased fields, check if the alias is present
        for field_name, field_info in self.model_fields.items():
            if field_info.alias == key:
                if field_name in self.model_fields_set:
                    return True
                if field_info.default is not None:
                    return True
        return False

    def get(self, key: str, default: Any = None) -> Any:
        """
        Get an item from the model, with a default value if the key does not exist.
        """
        if key in self:
            return self[key]
        # Attempt to get by alias if direct key fails
        for field_name, field_info in self.model_fields.items():
            if field_info.alias == key and field_name in self:
                return getattr(self, field_name)
        return default

    model_config = {  # Pydantic v2 config
        "populate_by_name": True,  # Allow using field names or aliases
        "extra": "allow",  # Allow extra fields not defined in the model
    }


class TopLogprob(Subscriptable):
    """
    Represents the log probabilities of a token chosen as the top choice.
    """

    token: str
    """
    The token.
    """
    bytes: Optional[List[int]] = None
    """
    A list of integers representing the UTF-8 bytes representation of the token.
    Useful in instances where characters are represented by multiple tokens.
    """
    logprob: float
    """
    The log probability of this token.
    """


class TokenLogprob(Subscriptable):
    """
    Represents the logprobs of a specific token, including top alternatives.
    """

    token: str
    """
    The token.
    """
    bytes: Optional[List[int]] = None
    """
    A list of integers representing the UTF-8 bytes representation of the token.
    Useful in instances where characters are represented by multiple tokens.
    """
    logprob: float
    """
    The log probability of this token.
    """
    top_logprobs: List[TopLogprob]
    """
    List of the most likely tokens and their logprobs, at this token position.
    In rare cases, there may be fewer than `top_logprobs` returned.
    """


class ChoiceLogprobs(Subscriptable):
    """
    Log probability information for the choice.
    """

    content: Optional[List[TokenLogprob]] = None
    """
    A list of message content tokens with log probability information.
    """
    # 'refusal' field is not explicitly listed in the latest common API responses for ChoiceLogprobs.
    # It might be specific to certain models or older API versions.
    # If it's still relevant for specific use cases, it can be added back.
    # refusal: Optional[List[TokenLogprob]] = None


class CompletionFunction(Subscriptable):  # Corresponds to FunctionCall in the response
    """
    The function that the model wants to call.
    (Deprecated in favor of CompletionToolCall)
    """

    name: str
    """
    The name of the function to be called.
    """
    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON
    format.
    """


class CompletionToolCallFunction(
    Subscriptable
):  # More specific than CompletionFunction
    """
    The function definition within a tool call in a completion response.
    """

    name: str
    """
    The name of the function to call.
    """
    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON
    format.
    """


class CompletionToolCall(Subscriptable):
    """
    A tool call in a completion response or chunk.
    """

    id: str
    """
    The ID of the tool call.
    """
    type: Literal["function"]
    """
    The type of the tool. Currently, only `function` is supported.
    """
    function: CompletionToolCallFunction
    """
    The function that the tool calls.
    """
    index: Optional[int] = None  # Present in streaming delta
    """
    The index of the tool call in the list of tool calls. (Streamed delta only)
    """


class CompletionMessage(Subscriptable):
    """
    A message object that forms part of the completion response.
    This usually represents the assistant's reply.
    """

    role: Literal["assistant"]
    """
    The role of the message author. Always `assistant` for completion messages.
    """
    content: Optional[
        Union[str, List[CompletionMessageContentPart]]
    ]  # Content can be null if tool_calls is present
    """
    The content of the message. Can be a string or an array of content parts.
    Can be null if tool_calls are present.
    """
    # name: Optional[str] = None # 'name' is not typically part of the assistant's response message.
    function_call: Optional[CompletionFunction] = None  # Deprecated
    """
    (Deprecated) The name and arguments of a function that should be called, as generated by the model.
    Replaced by `tool_calls`.
    """
    tool_calls: Optional[List[CompletionToolCall]] = None
    """
    The tool calls generated by the model, such as function calls.
    """
    # tool_call_id is not part of the assistant's response message itself, but rather for tool messages.


class CompletionUsage(Subscriptable):
    """
    Usage statistics for the completion request.
    """

    completion_tokens: int
    """Number of tokens in the generated completion."""
    prompt_tokens: int
    """Number of tokens in the prompt."""
    total_tokens: int
    """Total number of tokens used in the request (prompt + completion)."""


class CompletionResponse(Subscriptable):
    """
    A pydantic model representing a chat completion
    response.
    """

    class Choice(Subscriptable):
        """
        A completion choice.
        """

        message: CompletionMessage
        """
        A chat completion message generated by the model.
        """
        finish_reason: Literal[
            "stop",
            "length",
            "tool_calls",
            "content_filter",
            "function_call",
        ]  # "function_call" is deprecated
        """
        The reason the model stopped generating tokens.
        - `stop`: API returned complete message, or a message terminated by a stop sequence.
        - `length`: Incomplete model output due to `max_tokens` parameter or token limit.
        - `tool_calls`: Model called a tool.
        - `content_filter`: Omitted content due to a flag from our content filters.
        - `function_call`: (Deprecated) Model called a function.
        """
        index: int
        """
        The index of this choice in the list of choices.
        """
        logprobs: Optional[ChoiceLogprobs] = None
        """
        Log probability information for the choice.
        """

    id: str
    """
    A unique identifier for the chat completion.
    """
    choices: List[Choice]
    """
    A list of chat completion choices. Can be more than one if `n` is greater
    than 1.
    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion was created.
    """
    model: str
    """
    The model used for the chat completion.
    """
    object: Literal["chat.completion"]  # OpenAI SDK uses 'chat.completion'
    """
    The object type, which is always `chat.completion`.
    """
    system_fingerprint: Optional[str] = None
    """
    This fingerprint represents the backend configuration that the model runs with.
    You can use this value to track changes in the backend before comparing output
    from different API calls.
    """
    usage: Optional[CompletionUsage] = None
    """
    Usage statistics for the completion request.
    """



# ----------------------------------------------------------------------------
# Streaming
# ----------------------------------------------------------------------------


class CompletionResponseChunk(Subscriptable):
    """
    A pydantic model representing a chat completion stream chunk.
    """

    class Choice(Subscriptable):
        """
        A choice in a completion chunk.
        """

        class Delta(
            Subscriptable
        ):  
            """
            The delta message content for a streaming choice.
            """

            role: Optional[Literal["system", "user", "assistant", "tool"]] = (
                None  # Role appears in first chunk
            )
            """
            The role of the author of this message.
            """
            content: Optional[str] = None  # Content is streamed token by token
            """
            The contents of the chunk message.
            """
            tool_calls: Optional[List[CompletionToolCall]] = (
                None  # Tool calls can also be streamed
            )
            """
            Tool calls generated by the model. Can appear incrementally.
            Each tool call in the list can have an `index` field.
            """
            function_call: Optional[CompletionFunction] = None  # Deprecated
            """
            (Deprecated) The name and arguments of a function that should be called.
            """

        delta: Delta
        """
        A chat completion delta generated by streamed model responses.
        """
        finish_reason: Optional[
            Literal[
                "stop",
                "length",
                "tool_calls",
                "content_filter",
                "function_call",
            ]
        ] = None  # "function_call" is deprecated
        """
        The reason the model stopped generating tokens.
        Present in the final chunk of a choice.
        """
        index: int
        """
        The index of this choice in the stream.
        """
        logprobs: Optional[ChoiceLogprobs] = None  # Logprobs can also be part of chunks
        """
        Log probability information for the choice.
        """

    id: str
    """
    A unique identifier for the chat completion chunk.
    """
    choices: List[Choice]
    """
    A list of chat completion choices. Can be more than one if `n` is greater
    than 1.
    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion chunk was created.
    """
    model: str
    """
    The model to generate the completion.
    """
    object: Literal["chat.completion.chunk"]  # OpenAI SDK uses 'chat.completion.chunk'
    """
    The object type, which is always `chat.completion.chunk`.
    """
    system_fingerprint: Optional[str] = None
    """
    This fingerprint represents the backend configuration that the model runs with.
    You can use this value to track changes in the backend before comparing output
    from different API calls.
    """
    usage: Optional[CompletionUsage] = (
        None  # Usage is often null in chunks until the very end, or not present at all.
    )
    """
    An optional field that appears when the model is `gpt-3.5-turbo-0125` or `gpt-4-turbo-preview`
    and the `stream_options` parameter is set.
    It contains token usage statistics for the entire request, summed across all chunks.
    """


__all__ = [
    "CompletionResponse",
    "CompletionResponseChunk",
]   